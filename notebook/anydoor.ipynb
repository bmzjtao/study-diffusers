{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnyDoor+Segment-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zjt/anaconda3/envs/tf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Loaded model config from [configs/anydoor.yaml]\n",
      "Loaded state_dict from [/home/zjt/model/iic/AnyDoor/epoch=1-step=8687.ckpt]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "from PIL import Image, ImageDraw\n",
    "from omegaconf import OmegaConf\n",
    "from run_inference import inference_single_image\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "sam_checkpoint = \"/home/zjt/model/sam_vit_h_4b8939.pth\"\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
    "sam.to(\"cuda\")\n",
    "predictor = SamPredictor(sam)\n",
    "DConf = OmegaConf.load('./configs/datasets.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  5.00it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 点击图片时画个点在上面\n",
    "def on_click_image(image, global_state, evt: gr.SelectData):\n",
    "    radius_scale= 0.01\n",
    "    p_color = (255, 0, 0)\n",
    "    xy = evt.index\n",
    "    if global_state['img'] is None:\n",
    "        global_state['img'] = image\n",
    "    global_state['points'].append(xy)\n",
    "    overlay_rgba = Image.new(\"RGBA\", image.size, 0)\n",
    "    overlay_draw = ImageDraw.Draw(overlay_rgba)\n",
    "    rad_draw = int(image.size[0] * radius_scale)\n",
    "    for p in global_state['points']:\n",
    "        p_draw = int(p[0]), int(p[1])\n",
    "        overlay_draw.ellipse(\n",
    "            (\n",
    "                p_draw[0] - rad_draw,\n",
    "                p_draw[1] - rad_draw,\n",
    "                p_draw[0] + rad_draw,\n",
    "                p_draw[1] + rad_draw,\n",
    "            ),\n",
    "            fill=p_color,\n",
    "        )\n",
    "    image_draw =  Image.alpha_composite(image.convert(\"RGBA\"), overlay_rgba).convert(\"RGB\")\n",
    "    return image_draw,global_state\n",
    "# 推理时画上mask\n",
    "def run_predict(img,global_state):\n",
    "    image = np.array(global_state['img'])\n",
    "    predictor.set_image(image)\n",
    "    input_point = np.array(global_state['points'])\n",
    "    input_label = np.array([1]*len(global_state['points']))\n",
    "    if global_state['mask_input'] is None:\n",
    "        masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "        global_state['mask_input'] = logits[np.argmax(scores), :, :]\n",
    "    else:\n",
    "        masks, _, _ = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        mask_input=global_state['mask_input'][None, :, :],\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    global_state['mask'] = masks[0]\n",
    "    im_mask = np.uint8(masks[0] * 255)\n",
    "    im_mask = Image.fromarray(im_mask)\n",
    "    image_draw = Image.blend(img, im_mask.convert('RGB'), 0.5)\n",
    "    return image_draw,global_state\n",
    "# \n",
    "def run2(gl1,gl2):\n",
    "    ref_image = gl1['img']\n",
    "    ref_image = np.array(ref_image)\n",
    "    gt_image = gl2['img']\n",
    "    gt_image = np.array(gt_image)\n",
    "    ref_mask = gl1['mask'].astype(np.uint8)\n",
    "    tar_mask = gl2['mask'].astype(np.uint8)\n",
    "    gen_image = inference_single_image(ref_image, ref_mask, gt_image.copy(), tar_mask)\n",
    "    out = Image.fromarray(gen_image)\n",
    "    return out\n",
    "   \n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                global_state = gr.State({'img':None,'points':[],'mask_input':None,'mask':None})\n",
    "                im = gr.Image(type='pil',value='/home/zjt/workspace/img2img/AnyDoor/examples/Gradio/FG/10036.jpg')\n",
    "                \n",
    "                im.select(\n",
    "                    on_click_image,\n",
    "                    inputs=[im, global_state],\n",
    "                    outputs=[im,global_state],\n",
    "                    queue=False,\n",
    "                )\n",
    "                btn = gr.Button()\n",
    "                btn.click(fn=run_predict,inputs=[im,global_state],outputs=[im,global_state])\n",
    "            with gr.Column():\n",
    "                global_state2 = gr.State({'img':None,'points':[],'mask_input':None,'mask':None})\n",
    "                im2 = gr.Image(type='pil',value='output.png')\n",
    "                \n",
    "                im2.select(\n",
    "                    on_click_image,\n",
    "                    inputs=[im2, global_state2],\n",
    "                    outputs=[im2,global_state2],\n",
    "                    queue=False,\n",
    "                )\n",
    "                btn2 = gr.Button()\n",
    "                btn2.click(fn=run_predict,inputs=[im2,global_state2],outputs=[im2,global_state2])\n",
    "    baseline_gallery = gr.Image()\n",
    "    btn3 = gr.Button()\n",
    "    btn3.click(fn=run2,inputs=[global_state,global_state2],outputs=[baseline_gallery])\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
